{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Boston Housing Predictor - Phase 2: Advanced Feature Engineering\n",
        "\n",
        "## Overview\n",
        "Phase 2 focuses on improving generalization under temporal drift while strictly maintaining\n",
        "the chronological split (first 70% train, last 30% test). We:\n",
        "\n",
        "- Use stability analysis to select stable features\n",
        "- Apply train-quantile winsorization and monotonic transforms\n",
        "- Engineer ratio/difference interactions among stable features\n",
        "- Calibrate predictions using train-only bias/trend models\n",
        "- Tune SVR/GB with time-aware CV on the train segment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.linear_model import Ridge, ElasticNet\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)\n",
        "print('Libraries imported')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Strict Constraint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data: (490, 13), censored removed: 16\n",
            "Train: 343 | Test: 147 (strict)\n"
          ]
        }
      ],
      "source": [
        "columns = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']\n",
        "data = pd.read_csv('data/housing.csv', names=columns, delim_whitespace=True)\n",
        "censored = (data['MEDV'] >= 50.0).sum()\n",
        "data = data[data['MEDV'] < 50.0].copy()\n",
        "X = data.drop('MEDV', axis=1)\n",
        "y = data['MEDV']\n",
        "print(f'Data: {X.shape}, censored removed: {censored}')\n",
        "\n",
        "def strict_chronological_split(X, y, train_size=0.7):\n",
        "    split = int(len(X)*train_size)\n",
        "    return X.iloc[:split], X.iloc[split:], y.iloc[:split], y.iloc[split:]\n",
        "\n",
        "X_train, X_test, y_train, y_test = strict_chronological_split(X, y)\n",
        "print(f'Train: {X_train.shape[0]} | Test: {X_test.shape[0]} (strict)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Stability Analysis and Stable Feature Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stable features: ['RM', 'AGE', 'LSTAT', 'ZN', 'DIS', 'NOX', 'INDUS', 'PTRATIO']\n"
          ]
        }
      ],
      "source": [
        "def analyze_feature_stability(X, n_splits=10):\n",
        "    n = len(X)\n",
        "    size = n // n_splits\n",
        "    out = {}\n",
        "    for col in X.columns:\n",
        "        if col == 'CHAS':\n",
        "            continue\n",
        "        means, stds = [], []\n",
        "        for i in range(n_splits):\n",
        "            s = i*size\n",
        "            e = s+size if i < n_splits-1 else n\n",
        "            seg = X.iloc[s:e][col]\n",
        "            means.append(seg.mean())\n",
        "            stds.append(seg.std())\n",
        "        mv = np.var(means)\n",
        "        sv = np.var(stds)\n",
        "        ov = X[col].var()\n",
        "        r_mv = mv/ov if ov>0 else 0\n",
        "        r_sv = sv/ov if ov>0 else 0\n",
        "        out[col] = r_mv + r_sv\n",
        "    return pd.Series(out).sort_values()\n",
        "\n",
        "stability = analyze_feature_stability(X)\n",
        "stable_features = stability.head(8).index.tolist()\n",
        "print('Stable features:', stable_features)\n",
        "X_train_s = X_train[stable_features].copy()\n",
        "X_test_s = X_test[stable_features].copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Phase 2 Feature Transforms (Train stats only)\n",
        "- Winsorization by train quantiles (e.g., 1stâ€“99th)\n",
        "- Monotonic transforms for skew (log1p)\n",
        "- Ratio/difference features among stable signals\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformed train/test shapes: (343, 19) (147, 19)\n",
            "Scaled shapes: (343, 19) (147, 19)\n"
          ]
        }
      ],
      "source": [
        "def winsorize_by_train_quantiles(X_tr, X_te, lower=0.01, upper=0.99):\n",
        "    Xtr = X_tr.copy()\n",
        "    Xte = X_te.copy()\n",
        "    qs = Xtr.quantile([lower, upper])\n",
        "    for c in Xtr.columns:\n",
        "        low, high = qs.loc[lower, c], qs.loc[upper, c]\n",
        "        Xtr[c] = Xtr[c].clip(lower=low, upper=high)\n",
        "        Xte[c] = Xte[c].clip(lower=low, upper=high)\n",
        "    return Xtr, Xte\n",
        "\n",
        "def monotonic_transforms(X_tr, X_te, log_candidates=None):\n",
        "    Xtr = X_tr.copy()\n",
        "    Xte = X_te.copy()\n",
        "    if log_candidates is None:\n",
        "        log_candidates = [c for c in Xtr.columns if (Xtr[c] > 0).all() and (Xte[c] > 0).all()]\n",
        "    for c in log_candidates:\n",
        "        Xtr[c+'_log1p'] = np.log1p(Xtr[c])\n",
        "        Xte[c+'_log1p'] = np.log1p(Xte[c])\n",
        "    return Xtr, Xte\n",
        "\n",
        "def ratio_diff_features(X_tr, X_te, pairs):\n",
        "    Xtr = X_tr.copy()\n",
        "    Xte = X_te.copy()\n",
        "    for a, b in pairs:\n",
        "        if a in Xtr.columns and b in Xtr.columns:\n",
        "            denom_tr = np.where(Xtr[b]==0, 1e-6, Xtr[b])\n",
        "            denom_te = np.where(Xte[b]==0, 1e-6, Xte[b])\n",
        "            Xtr[f'{a}_over_{b}'] = Xtr[a] / denom_tr\n",
        "            Xte[f'{a}_over_{b}'] = Xte[a] / denom_te\n",
        "            Xtr[f'{a}_minus_{b}'] = Xtr[a] - Xtr[b]\n",
        "            Xte[f'{a}_minus_{b}'] = Xte[a] - Xte[b]\n",
        "    return Xtr, Xte\n",
        "\n",
        "# Apply transforms\n",
        "Xtr_w, Xte_w = winsorize_by_train_quantiles(X_train_s, X_test_s)\n",
        "Xtr_m, Xte_m = monotonic_transforms(Xtr_w, Xte_w)\n",
        "pairs = [('LSTAT','RM'), ('NOX','DIS')]\n",
        "Xtr_f, Xte_f = ratio_diff_features(Xtr_m, Xte_m, pairs)\n",
        "print('Transformed train/test shapes:', Xtr_f.shape, Xte_f.shape)\n",
        "\n",
        "# Scale with train-only stats\n",
        "scaler = RobustScaler()\n",
        "Xtr = scaler.fit_transform(Xtr_f)\n",
        "Xte = scaler.transform(Xte_f)\n",
        "print('Scaled shapes:', Xtr.shape, Xte.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Time-aware CV on Train and Hyperparameter Tuning\n",
        "We use forward-style splits within the first 70% to tune models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best SVR: (np.float64(3.3071231688475473), {'C': 10, 'gamma': 0.05})\n",
            "Best GB: (np.float64(3.0402137080134883), {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 200})\n"
          ]
        }
      ],
      "source": [
        "def forward_cv_indices(n, k=5):\n",
        "    # Expanding window CV within train portion\n",
        "    sizes = np.linspace(0.6, 0.95, k)\n",
        "    idx = np.arange(n)\n",
        "    folds = []\n",
        "    for s in sizes:\n",
        "        split = int(n*s)\n",
        "        if split < n-1:\n",
        "            folds.append((idx[:split], idx[split:]))\n",
        "    return folds\n",
        "\n",
        "def tune_model(model_name, Xtr, ytr):\n",
        "    results = []\n",
        "    folds = forward_cv_indices(len(Xtr), k=5)\n",
        "    if model_name == 'SVR':\n",
        "        grid = ParameterGrid({'C':[0.5,1,3,10], 'gamma':['scale',0.05,0.1,0.2]})\n",
        "        base = SVR(kernel='rbf')\n",
        "    elif model_name == 'GB':\n",
        "        grid = ParameterGrid({'n_estimators':[200,300,500], 'learning_rate':[0.03,0.05], 'max_depth':[2,3]})\n",
        "        base = GradientBoostingRegressor(random_state=42)\n",
        "    else:\n",
        "        return None\n",
        "    for params in grid:\n",
        "        rmses = []\n",
        "        for tr, va in folds:\n",
        "            model = base.set_params(**params)\n",
        "            model.fit(Xtr[tr], ytr.iloc[tr])\n",
        "            pred = model.predict(Xtr[va])\n",
        "            rmses.append(np.sqrt(mean_squared_error(ytr.iloc[va], pred)))\n",
        "        results.append((np.mean(rmses), params))\n",
        "    results.sort(key=lambda x: x[0])\n",
        "    return results[0] if results else None\n",
        "\n",
        "best_svr = tune_model('SVR', Xtr, y_train)\n",
        "best_gb = tune_model('GB', Xtr, y_train)\n",
        "print('Best SVR:', best_svr)\n",
        "print('Best GB:', best_gb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train Final Models and Target Calibration (train-only)\n",
        "We fit models on full train, then learn a bias/trend calibrator on train residuals and apply to test predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        model  rmse_raw    r2_raw  rmse_cal    r2_cal\n",
            "0         SVR  3.814822  0.506698  3.754015  0.522299\n",
            "1          GB  6.000421 -0.220472  5.954715 -0.201950\n",
            "2       Ridge  5.903238 -0.181259  5.880982 -0.172368\n",
            "3  ElasticNet  5.558285 -0.047240  5.542944 -0.041467\n"
          ]
        }
      ],
      "source": [
        "def train_final(model_name, best, Xtr, ytr):\n",
        "    if model_name=='SVR':\n",
        "        model = SVR(kernel='rbf', **best[1]) if best else SVR(kernel='rbf')\n",
        "    elif model_name=='GB':\n",
        "        params = best[1] if best else {'n_estimators':300,'learning_rate':0.03,'max_depth':3}\n",
        "        model = GradientBoostingRegressor(random_state=42, **params)\n",
        "    elif model_name=='Ridge':\n",
        "        model = Ridge(alpha=1.0)\n",
        "    else:\n",
        "        model = ElasticNet(alpha=0.001, l1_ratio=0.5)\n",
        "    model.fit(Xtr, ytr)\n",
        "    return model\n",
        "\n",
        "svr = train_final('SVR', best_svr, Xtr, y_train)\n",
        "gb = train_final('GB', best_gb, Xtr, y_train)\n",
        "ridge = train_final('Ridge', None, Xtr, y_train)\n",
        "enet = train_final('EN', None, Xtr, y_train)\n",
        "\n",
        "def fit_calibrator(y_true, y_pred):\n",
        "    # Simple linear calibrator on train residual trend: y = a*y_pred + b\n",
        "    Xc = np.vstack([y_pred, np.ones_like(y_pred)]).T\n",
        "    a, b = np.linalg.lstsq(Xc, y_true, rcond=None)[0]\n",
        "    return a, b\n",
        "\n",
        "def apply_calibrator(y_pred, a, b):\n",
        "    return a*y_pred + b\n",
        "\n",
        "models = {'SVR': svr, 'GB': gb, 'Ridge': ridge, 'ElasticNet': enet}\n",
        "results = []\n",
        "for name, m in models.items():\n",
        "    # Train predictions for calibration\n",
        "    pred_tr = m.predict(Xtr)\n",
        "    a, b = fit_calibrator(y_train.values, pred_tr)\n",
        "    # Test predictions (before/after calibration)\n",
        "    pred_te = m.predict(Xte)\n",
        "    pred_te_cal = apply_calibrator(pred_te, a, b)\n",
        "    # Metrics\n",
        "    rmse_raw = np.sqrt(mean_squared_error(y_test, pred_te))\n",
        "    r2_raw = r2_score(y_test, pred_te)\n",
        "    rmse_cal = np.sqrt(mean_squared_error(y_test, pred_te_cal))\n",
        "    r2_cal = r2_score(y_test, pred_te_cal)\n",
        "    results.append({'model':name,'rmse_raw':rmse_raw,'r2_raw':r2_raw,'rmse_cal':rmse_cal,'r2_cal':r2_cal})\n",
        "df_res = pd.DataFrame(results)\n",
        "print(df_res)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary\n",
        "We report raw and calibrated metrics on the strict test set and compare improvements."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
