{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c9b1f15",
   "metadata": {},
   "source": [
    "# Boston Housing Case Study: Temporal vs Random Split Analysis\n",
    "\n",
    "This notebook implements the case study requirements:\n",
    "- Train a regression model using the first 70% of data (older houses)\n",
    "- Evaluate on the last 30% (newer houses)\n",
    "- Compare results to a random split baseline\n",
    "- Analyze why performance may differ across the two splits\n",
    "- Suggest ways to improve robustness to temporal changes\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "The Boston Housing Dataset contains 506 samples with 13 features:\n",
    "- **Features**: CRIM, ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, B, LSTAT\n",
    "- **Target**: MEDV (median house value in $1000s)\n",
    "\n",
    "Note: Some MEDV values are censored at 50.0 (corresponding to $50,000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd60874",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dcac0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa51f491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "data = pd.read_csv('../data/housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Features: {list(data.columns[:-1])}\")\n",
    "print(f\"Target: {data.columns[-1]}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83ec90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"Dataset Info:\")\n",
    "print(f\"Total samples: {len(data)}\")\n",
    "print(f\"Features: {data.shape[1]-1}\")\n",
    "print(f\"Target variable: {data.columns[-1]}\")\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(data['MEDV'].describe())\n",
    "print(f\"\\nCensored values (MEDV >= 50): {(data['MEDV'] >= 50).sum()} ({(data['MEDV'] >= 50).sum()/len(data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0131ba97",
   "metadata": {},
   "source": [
    "## 2. Quick EDA (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0d0720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nData types:\")\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50ce6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of target variable\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogram\n",
    "ax1.hist(data['MEDV'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax1.axvline(data['MEDV'].median(), color='red', linestyle='--', label=f'Median: {data[\"MEDV\"].median():.1f}')\n",
    "ax1.axvline(50, color='orange', linestyle='--', label='Censoring threshold (50)')\n",
    "ax1.set_xlabel('MEDV (Median House Value in $1000s)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Distribution of Target Variable (MEDV)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "ax2.boxplot(data['MEDV'])\n",
    "ax2.set_ylabel('MEDV (Median House Value in $1000s)')\n",
    "ax2.set_title('Box Plot of Target Variable')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ccd135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = data.corr()\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show top correlations with MEDV\n",
    "medv_correlations = correlation_matrix['MEDV'].abs().sort_values(ascending=False)\n",
    "print(\"Top correlations with MEDV:\")\n",
    "print(medv_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c58dad",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Split Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468b630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two versions of the dataset as per design\n",
    "# Version 1: Keep all rows (including censored values)\n",
    "data_all = data.copy()\n",
    "\n",
    "# Version 2: Remove rows with MEDV >= 50.0 (censored values)\n",
    "data_clean = data[data['MEDV'] < 50.0].copy()\n",
    "\n",
    "print(f\"Version 1 (Keep all): {data_all.shape[0]} samples\")\n",
    "print(f\"Version 2 (Remove censored): {data_clean.shape[0]} samples\")\n",
    "print(f\"Removed {data_all.shape[0] - data_clean.shape[0]} censored samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4aea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define split functions\n",
    "def create_chronological_split(data, train_ratio=0.7):\n",
    "    \"\"\"Create chronological split (first 70% vs last 30%)\"\"\"\n",
    "    split_idx = int(len(data) * train_ratio)\n",
    "    train_data = data.iloc[:split_idx]\n",
    "    test_data = data.iloc[split_idx:]\n",
    "    return train_data, test_data\n",
    "\n",
    "def create_age_based_split(data, train_ratio=0.7):\n",
    "    \"\"\"Create AGE-descending split (top 70% AGE vs bottom 30% AGE)\"\"\"\n",
    "    data_sorted = data.sort_values('AGE', ascending=False)\n",
    "    split_idx = int(len(data_sorted) * train_ratio)\n",
    "    train_data = data_sorted.iloc[:split_idx]\n",
    "    test_data = data_sorted.iloc[split_idx:]\n",
    "    return train_data, test_data\n",
    "\n",
    "def create_random_split(data, train_ratio=0.7, random_state=RANDOM_STATE):\n",
    "    \"\"\"Create random split baseline\"\"\"\n",
    "    train_data, test_data = train_test_split(data, train_size=train_ratio, \n",
    "                                            random_state=random_state, shuffle=True)\n",
    "    return train_data, test_data\n",
    "\n",
    "def prepare_features_target(data, scale_features=True):\n",
    "    \"\"\"Prepare features and target, optionally scale features\"\"\"\n",
    "    X = data.iloc[:, :-1]  # All features except MEDV\n",
    "    y = data.iloc[:, -1]   # MEDV\n",
    "    \n",
    "    if scale_features:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        return X_scaled, y, scaler\n",
    "    else:\n",
    "        return X, y, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee3c5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create splits for both dataset versions\n",
    "print(\"Creating splits for both dataset versions...\")\n",
    "\n",
    "# Version 1: Keep all data\n",
    "print(\"\\n=== Version 1: Keep All Data ===\")\n",
    "chrono_train_all, chrono_test_all = create_chronological_split(data_all)\n",
    "age_train_all, age_test_all = create_age_based_split(data_all)\n",
    "random_train_all, random_test_all = create_random_split(data_all)\n",
    "\n",
    "print(f\"Chronological split: {len(chrono_train_all)} train, {len(chrono_test_all)} test\")\n",
    "print(f\"AGE-based split: {len(age_train_all)} train, {len(age_test_all)} test\")\n",
    "print(f\"Random split: {len(random_train_all)} train, {len(random_test_all)} test\")\n",
    "\n",
    "# Version 2: Remove censored data\n",
    "print(\"\\n=== Version 2: Remove Censored Data ===\")\n",
    "chrono_train_clean, chrono_test_clean = create_chronological_split(data_clean)\n",
    "age_train_clean, age_test_clean = create_age_based_split(data_clean)\n",
    "random_train_clean, random_test_clean = create_random_split(data_clean)\n",
    "\n",
    "print(f\"Chronological split: {len(chrono_train_clean)} train, {len(chrono_test_clean)} test\")\n",
    "print(f\"AGE-based split: {len(age_train_clean)} train, {len(age_test_clean)} test\")\n",
    "print(f\"Random split: {len(random_train_clean)} train, {len(random_test_clean)} test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cd3594",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3f34ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation function\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name=\"Model\"):\n",
    "    \"\"\"Train and evaluate a model, returning metrics\"\"\"\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2\n",
    "    }\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Ridge': Ridge(alpha=1.0, random_state=RANDOM_STATE),\n",
    "    'GradientBoosting': GradientBoostingRegressor(\n",
    "        n_estimators=100, learning_rate=0.1, max_depth=3, \n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109b3cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models on all splits and dataset versions\n",
    "results = []\n",
    "\n",
    "print(\"Evaluating models on different splits and dataset versions...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Version 1: Keep all data\n",
    "print(\"\\nVERSION 1: Keep All Data (including censored)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for split_name, (train_data, test_data) in [\n",
    "    ('Chronological', (chrono_train_all, chrono_test_all)),\n",
    "    ('AGE-based', (age_train_all, age_test_all)),\n",
    "    ('Random', (random_train_all, random_test_all))\n",
    "]:\n",
    "    print(f\"\\n{split_name} Split:\")\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X_train, y_train, scaler = prepare_features_target(train_data, scale_features=True)\n",
    "    X_test, y_test, _ = prepare_features_target(test_data, scale_features=False)\n",
    "    \n",
    "    # Scale test features using train scaler\n",
    "    if scaler is not None:\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Evaluate each model\n",
    "    for model_name, model in models.items():\n",
    "        # For tree-based models, don't scale features\n",
    "        if model_name == 'GradientBoosting':\n",
    "            X_train_raw, y_train_raw, _ = prepare_features_target(train_data, scale_features=False)\n",
    "            X_test_raw, y_test_raw, _ = prepare_features_target(test_data, scale_features=False)\n",
    "            result = evaluate_model(model, X_train_raw, X_test_raw, y_train_raw, y_test_raw, model_name)\n",
    "        else:\n",
    "            result = evaluate_model(model, X_train, X_test, y_train, y_test, model_name)\n",
    "        \n",
    "        result['dataset_version'] = 'Keep All'\n",
    "        result['split_type'] = split_name\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"  {model_name}: RMSE={result['rmse']:.3f}, MAE={result['mae']:.3f}, R²={result['r2']:.3f}\")\n",
    "\n",
    "# Version 2: Remove censored data\n",
    "print(\"\\n\\nVERSION 2: Remove Censored Data\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for split_name, (train_data, test_data) in [\n",
    "    ('Chronological', (chrono_train_clean, chrono_test_clean)),\n",
    "    ('AGE-based', (age_train_clean, age_test_clean)),\n",
    "    ('Random', (random_train_clean, random_test_clean))\n",
    "]:\n",
    "    print(f\"\\n{split_name} Split:\")\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X_train, y_train, scaler = prepare_features_target(train_data, scale_features=True)\n",
    "    X_test, y_test, _ = prepare_features_target(test_data, scale_features=False)\n",
    "    \n",
    "    # Scale test features using train scaler\n",
    "    if scaler is not None:\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Evaluate each model\n",
    "    for model_name, model in models.items():\n",
    "        # For tree-based models, don't scale features\n",
    "        if model_name == 'GradientBoosting':\n",
    "            X_train_raw, y_train_raw, _ = prepare_features_target(train_data, scale_features=False)\n",
    "            X_test_raw, y_test_raw, _ = prepare_features_target(test_data, scale_features=False)\n",
    "            result = evaluate_model(model, X_train_raw, X_test_raw, y_train_raw, y_test_raw, model_name)\n",
    "        else:\n",
    "            result = evaluate_model(model, X_train, X_test, y_train, y_test, model_name)\n",
    "        \n",
    "        result['dataset_version'] = 'Remove Censored'\n",
    "        result['split_type'] = split_name\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"  {model_name}: RMSE={result['rmse']:.3f}, MAE={result['mae']:.3f}, R²={result['r2']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beaa4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame for easier analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Complete Results Summary:\")\n",
    "print(\"=\" * 80)\n",
    "print(results_df.round(3))\n",
    "\n",
    "# Save results for later analysis\n",
    "results_df.to_csv('model_evaluation_results.csv', index=False)\n",
    "print(\"\\nResults saved to 'model_evaluation_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4d5029",
   "metadata": {},
   "source": [
    "## 5. Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899d10f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# 1. RMSE comparison across splits and models\n",
    "ax1 = axes[0]\n",
    "pivot_rmse = results_df.pivot_table(values='rmse', index='split_type', \n",
    "                                    columns=['model_name', 'dataset_version'], aggfunc='mean')\n",
    "pivot_rmse.plot(kind='bar', ax=ax1, color=['skyblue', 'lightcoral', 'lightgreen', 'orange'])\n",
    "ax1.set_title('RMSE Comparison Across Splits and Models')\n",
    "ax1.set_ylabel('RMSE')\n",
    "ax1.legend(title='Model + Dataset Version', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. R² comparison\n",
    "ax2 = axes[1]\n",
    "pivot_r2 = results_df.pivot_table(values='r2', index='split_type', \n",
    "                                  columns=['model_name', 'dataset_version'], aggfunc='mean')\n",
    "pivot_r2.plot(kind='bar', ax=ax2, color=['skyblue', 'lightcoral', 'lightgreen', 'orange'])\n",
    "ax2.set_title('R² Comparison Across Splits and Models')\n",
    "ax2.set_ylabel('R²')\n",
    "ax2.legend(title='Model + Dataset Version', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Model performance comparison (Ridge)\n",
    "ax3 = axes[2]\n",
    "ridge_results = results_df[results_df['model_name'] == 'Ridge']\n",
    "ridge_pivot = ridge_results.pivot_table(values='rmse', index='split_type', \n",
    "                                       columns='dataset_version', aggfunc='mean')\n",
    "ridge_pivot.plot(kind='bar', ax=ax3, color=['skyblue', 'lightcoral'])\n",
    "ax3.set_title('Ridge Model: RMSE by Split Type and Dataset Version')\n",
    "ax3.set_ylabel('RMSE')\n",
    "ax3.legend(title='Dataset Version')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Model performance comparison (GradientBoosting)\n",
    "ax4 = axes[3]\n",
    "gbr_results = results_df[results_df['model_name'] == 'GradientBoosting']\n",
    "gbr_pivot = gbr_results.pivot_table(values='rmse', index='split_type', \n",
    "                                    columns='dataset_version'], aggfunc='mean')\n",
    "gbr_pivot.plot(kind='bar', ax=ax4, color=['lightgreen', 'orange'])\n",
    "ax4.set_title('GradientBoosting Model: RMSE by Split Type and Dataset Version')\n",
    "ax4.set_ylabel('RMSE')\n",
    "ax4.legend(title='Dataset Version')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdba397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance gap analysis\n",
    "print(\"Performance Gap Analysis: Chronological vs Random Split\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for dataset_version in ['Keep All', 'Remove Censored']:\n",
    "    print(f\"\\n{dataset_version} Dataset:\")\n",
    "    \n",
    "    for model_name in ['Ridge', 'GradientBoosting']:\n",
    "        # Get results for chronological and random splits\n",
    "        chrono_result = results_df[(results_df['dataset_version'] == dataset_version) & \n",
    "                                   (results_df['split_type'] == 'Chronological') & \n",
    "                                   (results_df['model_name'] == model_name)].iloc[0]\n",
    "        \n",
    "        random_result = results_df[(results_df['dataset_version'] == dataset_version) & \n",
    "                                   (results_df['split_type'] == 'Random') & \n",
    "                                   (results_df['model_name'] == model_name)].iloc[0]\n",
    "        \n",
    "        # Calculate performance gap\n",
    "        rmse_gap = chrono_result['rmse'] - random_result['rmse']\n",
    "        r2_gap = random_result['r2'] - chrono_result['r2']\n",
    "        \n",
    "        print(f\"  {model_name}:\")\n",
    "        print(f\"    RMSE gap (Chrono - Random): {rmse_gap:+.3f}\")\n",
    "        print(f\"    R² gap (Random - Chrono): {r2_gap:+.3f}\")\n",
    "        \n",
    "        if rmse_gap > 0:\n",
    "            print(f\"    → Chronological split performs WORSE by {rmse_gap:.3f} RMSE\")\n",
    "        else:\n",
    "            print(f\"    → Chronological split performs BETTER by {abs(rmse_gap):.3f} RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dfa6d3",
   "metadata": {},
   "source": [
    "## 6. Drift and Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f4d3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature distribution shifts between train and test sets\n",
    "print(\"Feature Distribution Shift Analysis: Chronological Split\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Focus on key features as per design\n",
    "key_features = ['LSTAT', 'RM', 'NOX', 'DIS', 'RAD', 'TAX']\n",
    "\n",
    "for dataset_version, (train_data, test_data) in [\n",
    "    ('Keep All', (chrono_train_all, chrono_test_all)),\n",
    "    ('Remove Censored', (chrono_train_clean, chrono_test_clean))\n",
    "]:\n",
    "    print(f\"\\n{dataset_version} Dataset:\")\n",
    "    \n",
    "    for feature in key_features:\n",
    "        train_mean = train_data[feature].mean()\n",
    "        test_mean = test_data[feature].mean()\n",
    "        train_std = train_data[feature].std()\n",
    "        test_std = test_data[feature].std()\n",
    "        \n",
    "        mean_shift = test_mean - train_mean\n",
    "        std_shift = test_std - train_std\n",
    "        \n",
    "        print(f\"  {feature}:\")\n",
    "        print(f\"    Mean: {train_mean:.3f} → {test_mean:.3f} (shift: {mean_shift:+.3f})\")\n",
    "        print(f\"    Std:  {train_std:.3f} → {test_std:.3f} (shift: {std_shift:+.3f})\")\n",
    "        \n",
    "        # Statistical significance test\n",
    "        _, p_value = stats.ks_2samp(train_data[feature], test_data[feature])\n",
    "        print(f\"    KS test p-value: {p_value:.4f} {'(significant)' if p_value < 0.05 else '(not significant)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44018093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution shifts for key features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(key_features):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot distributions for both dataset versions\n",
    "    ax.hist(chrono_train_all[feature], bins=20, alpha=0.6, label='Train (Keep All)', \n",
    "             color='skyblue', density=True)\n",
    "    ax.hist(chrono_test_all[feature], bins=20, alpha=0.6, label='Test (Keep All)', \n",
    "             color='lightcoral', density=True)\n",
    "    \n",
    "    ax.set_title(f'{feature} Distribution Shift')\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164d5aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model explainability analysis\n",
    "print(\"Model Explainability Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Ridge model coefficients (using chronological split, keep all data)\n",
    "X_train, y_train, scaler = prepare_features_target(chrono_train_all, scale_features=True)\n",
    "ridge_model = Ridge(alpha=1.0, random_state=RANDOM_STATE)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = data.columns[:-1]\n",
    "\n",
    "# Plot Ridge coefficients\n",
    "plt.figure(figsize=(12, 6))\n",
    "coefficients = ridge_model.coef_\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "plt.barh(range(len(feature_importance)), feature_importance['Coefficient'], \n",
    "          color=['red' if x < 0 else 'blue' for x in feature_importance['Coefficient']])\n",
    "plt.yticks(range(len(feature_importance)), feature_importance['Feature'])\n",
    "plt.xlabel('Standardized Coefficient')\n",
    "plt.title('Ridge Model: Feature Coefficients (Chronological Split, Keep All Data)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top features by absolute coefficient magnitude:\")\n",
    "print(feature_importance.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2f13be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientBoosting feature importance\n",
    "X_train_raw, y_train_raw, _ = prepare_features_target(chrono_train_all, scale_features=False)\n",
    "gbr_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, \n",
    "                                      random_state=RANDOM_STATE)\n",
    "gbr_model.fit(X_train_raw, y_train_raw)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "feature_importance_gbr = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': gbr_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.barh(range(len(feature_importance_gbr)), feature_importance_gbr['Importance'], \n",
    "          color='lightgreen')\n",
    "plt.yticks(range(len(feature_importance_gbr)), feature_importance_gbr['Feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('GradientBoosting Model: Feature Importance (Chronological Split, Keep All Data)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top features by importance:\")\n",
    "print(feature_importance_gbr.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895b35e7",
   "metadata": {},
   "source": [
    "## 7. Findings and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d33f913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key findings\n",
    "print(\"KEY FINDINGS AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Performance comparison summary\n",
    "print(\"\\n1. PERFORMANCE COMPARISON:\")\n",
    "print(\"   - Chronological split performance vs Random split baseline\")\n",
    "print(\"   - Impact of censored data removal\")\n",
    "print(\"   - Model robustness across different splits\")\n",
    "\n",
    "# 2. Drift analysis summary\n",
    "print(\"\\n2. TEMPORAL DRIFT ANALYSIS:\")\n",
    "print(\"   - Feature distribution shifts between train and test sets\")\n",
    "print(\"   - Statistical significance of observed shifts\")\n",
    "print(\"   - Key features contributing to drift\")\n",
    "\n",
    "# 3. Model explainability\n",
    "print(\"\\n3. MODEL EXPLAINABILITY:\")\n",
    "print(\"   - Ridge model: Standardized coefficients\")\n",
    "print(\"   - GradientBoosting: Feature importance rankings\")\n",
    "print(\"   - Feature contribution to predictions\")\n",
    "\n",
    "# 4. Recommendations\n",
    "print(\"\\n4. ROBUSTNESS RECOMMENDATIONS:\")\n",
    "print(\"   - Regularized linear models for stability\")\n",
    "print(\"   - Periodic model retraining\")\n",
    "print(\"   - Monitor covariate drift on key features\")\n",
    "print(\"   - Time-aware validation strategies\")\n",
    "print(\"   - Ensemble methods for robustness\")\n",
    "\n",
    "# 5. Next steps\n",
    "print(\"\\n5. NEXT STEPS:\")\n",
    "print(\"   - Implement rolling window validation\")\n",
    "print(\"   - Test additional robustness techniques\")\n",
    "print(\"   - Deploy monitoring systems\")\n",
    "print(\"   - Establish retraining protocols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b921f572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive summary table\n",
    "print(\"COMPREHENSIVE RESULTS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Best performing configuration for each split type\n",
    "best_configs = results_df.loc[results_df.groupby('split_type')['rmse'].idxmin()]\n",
    "print(\"\\nBest Configuration for Each Split Type:\")\n",
    "print(best_configs[['split_type', 'model_name', 'dataset_version', 'rmse', 'mae', 'r2']].round(3))\n",
    "\n",
    "# Performance ranking\n",
    "print(\"\\nOverall Performance Ranking (by RMSE):\")\n",
    "performance_ranking = results_df.sort_values('rmse')[['split_type', 'model_name', 'dataset_version', 'rmse', 'mae', 'r2']]\n",
    "print(performance_ranking.round(3))\n",
    "\n",
    "# Save comprehensive results\n",
    "performance_ranking.to_csv('performance_ranking.csv', index=False)\n",
    "print(\"\\nPerformance ranking saved to 'performance_ranking.csv'\")\n",
    "\n",
    "# Final recommendations\n",
    "print(\"\\nFINAL RECOMMENDATIONS:\")\n",
    "print(\"1. Use Ridge regression with chronological split for temporal analysis\")\n",
    "print(\"2. Monitor feature drift on LSTAT, RM, NOX, DIS, RAD, TAX\")\n",
    "print(\"3. Implement regular retraining schedule\")\n",
    "print(\"4. Consider ensemble methods for improved robustness\")\n",
    "print(\"5. Establish drift detection and alerting systems\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "nbformat_minor": 4,
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
